models:
- endpoints:
    global:
      quotas:
        concurrent: null
        rpm:
          code: L-D5F39C2F
          name: Global cross-region model inference requests per minute for Amazon
            Nova 2 Lite
        tpd:
          code: L-AD940EDE
          name: Global cross-region model inference tokens per day for Amazon Nova
            2 Lite
        tpm:
          code: L-71C69B70
          name: Global cross-region model inference tokens per minute for Amazon Nova
            2 Lite
  inference_profiles:
  - global
  inference_types:
  - INFERENCE_PROFILE
  model_id: amazon.nova-2-lite-v1:0
  provider: Amazon
- endpoints:
    apac:
      quotas:
        concurrent: null
        rpm:
          code: L-89F8391A
          name: Cross-region model inference requests per minute for Amazon Nova Lite
        tpd: null
        tpm:
          code: L-7C42E72A
          name: Cross-region model inference tokens per minute for Amazon Nova Lite
  inference_profiles:
  - apac
  inference_types:
  - INFERENCE_PROFILE
  model_id: amazon.nova-lite-v1:0
  provider: Amazon
- endpoints:
    apac:
      quotas:
        concurrent: null
        rpm:
          code: L-3F110E0F
          name: Cross-region model inference requests per minute for Amazon Nova Micro
        tpd: null
        tpm:
          code: L-DC7FF66C
          name: Cross-region model inference tokens per minute for Amazon Nova Micro
  inference_profiles:
  - apac
  inference_types:
  - INFERENCE_PROFILE
  model_id: amazon.nova-micro-v1:0
  provider: Amazon
- endpoints:
    apac:
      quotas:
        concurrent: null
        rpm:
          code: L-ED46B8C5
          name: Cross-region model inference requests per minute for Amazon Nova Pro
        tpd: null
        tpm:
          code: L-C0326783
          name: Cross-region model inference tokens per minute for Amazon Nova Pro
  inference_profiles:
  - apac
  inference_types:
  - INFERENCE_PROFILE
  model_id: amazon.nova-pro-v1:0
  provider: Amazon
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-DF0E34D4
          name: On-demand model inference requests per minute for Amazon Titan Multimodal
            Embeddings G1
        tpd: null
        tpm:
          code: L-ABC24664
          name: On-demand model inference tokens per minute for Amazon Titan Multimodal
            Embeddings G1
  inference_types:
  - ON_DEMAND
  model_id: amazon.titan-embed-image-v1
  provider: Amazon
- inference_types:
  - PROVISIONED
  model_id: amazon.titan-embed-image-v1:0
  provider: Amazon
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-26C560CE
          name: On-demand model inference requests per minute for Amazon Titan Text
            Embeddings V2
        tpd: null
        tpm:
          code: L-DE641971
          name: On-demand model inference tokens per minute for Amazon Titan Text
            Embeddings V2
  inference_types:
  - ON_DEMAND
  model_id: amazon.titan-embed-text-v2:0
  provider: Amazon
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-A70F1DE3
          name: On-demand model inference requests per minute for Amazon Titan Text
            Lite
        tpd: null
        tpm:
          code: L-70BE83E9
          name: On-demand model inference tokens per minute for Amazon Titan Text
            Lite
  inference_types:
  - ON_DEMAND
  model_id: amazon.titan-text-lite-v1
  provider: Amazon
- inference_types:
  - PROVISIONED
  model_id: amazon.titan-text-lite-v1:0:4k
  provider: Amazon
- endpoints:
    apac:
      quotas:
        concurrent: null
        rpm:
          code: L-F457545D
          name: Cross-region model inference requests per minute for Anthropic Claude
            3.5 Sonnet
        tpd: null
        tpm:
          code: L-479B647F
          name: Cross-region model inference tokens per minute for Anthropic Claude
            3.5 Sonnet
  inference_profiles:
  - apac
  inference_types:
  - INFERENCE_PROFILE
  model_id: anthropic.claude-3-5-sonnet-20240620-v1:0
  provider: Anthropic
- endpoints:
    apac:
      quotas:
        concurrent: null
        rpm:
          code: L-1D3E59A3
          name: Cross-Region model inference requests per minute for Anthropic Claude
            3.5 Sonnet V2
        tpd: null
        tpm:
          code: L-FF8B4E28
          name: Cross-Region model inference tokens per minute for Anthropic Claude
            3.5 Sonnet V2
  inference_profiles:
  - apac
  inference_types:
  - INFERENCE_PROFILE
  model_id: anthropic.claude-3-5-sonnet-20241022-v2:0
  provider: Anthropic
- endpoints:
    apac:
      quotas:
        concurrent: null
        rpm:
          code: L-3D8CC480
          name: Cross-region model inference requests per minute for Anthropic Claude
            3.7 Sonnet V1
        tpd:
          code: L-9EB71894
          name: Model invocation max tokens per day for Anthropic Claude 3.7 Sonnet
            V1 (doubled for cross-region calls)
        tpm:
          code: L-6E888CC2
          name: Cross-region model inference tokens per minute for Anthropic Claude
            3.7 Sonnet V1
  inference_profiles:
  - apac
  inference_types:
  - INFERENCE_PROFILE
  model_id: anthropic.claude-3-7-sonnet-20250219-v1:0
  provider: Anthropic
- endpoints:
    apac:
      quotas:
        concurrent: null
        rpm:
          code: L-616A3F5B
          name: Cross-region model inference requests per minute for Anthropic Claude
            3 Haiku
        tpd: null
        tpm:
          code: L-DCADBC78
          name: Cross-region model inference tokens per minute for Anthropic Claude
            3 Haiku
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-2DC80978
          name: On-demand model inference requests per minute for Anthropic Claude
            3 Haiku
        tpd: null
        tpm:
          code: L-8CE99163
          name: On-demand model inference tokens per minute for Anthropic Claude 3
            Haiku
  inference_profiles:
  - apac
  inference_types:
  - ON_DEMAND
  model_id: anthropic.claude-3-haiku-20240307-v1:0
  provider: Anthropic
- inference_types:
  - PROVISIONED
  model_id: anthropic.claude-3-haiku-20240307-v1:0:48k
  provider: Anthropic
- endpoints:
    apac:
      quotas:
        concurrent: null
        rpm:
          code: L-46591118
          name: Cross-region model inference requests per minute for Anthropic Claude
            3 Sonnet
        tpd: null
        tpm:
          code: L-5DF13F64
          name: Cross-region model inference tokens per minute for Anthropic Claude
            3 Sonnet
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-F406804E
          name: On-demand model inference requests per minute for Anthropic Claude
            3 Sonnet
        tpd: null
        tpm:
          code: L-4C35BB2A
          name: On-demand model inference tokens per minute for Anthropic Claude 3
            Sonnet
  inference_profiles:
  - apac
  inference_types:
  - ON_DEMAND
  model_id: anthropic.claude-3-sonnet-20240229-v1:0
  provider: Anthropic
- inference_types:
  - PROVISIONED
  model_id: anthropic.claude-3-sonnet-20240229-v1:0:28k
  provider: Anthropic
- endpoints:
    global:
      quotas:
        concurrent: null
        rpm:
          code: L-E5084BBA
          name: Global cross-region model inference requests per minute for Anthropic
            Claude Haiku 4.5
        tpd:
          code: L-B5C049AE
          name: Global cross-region model inference tokens per day for Anthropic Claude
            Haiku 4.5
        tpm:
          code: L-9A11C666
          name: Global cross-region model inference tokens per minute for Anthropic
            Claude Haiku 4.5
  inference_profiles:
  - global
  inference_types:
  - INFERENCE_PROFILE
  model_id: anthropic.claude-haiku-4-5-20251001-v1:0
  provider: Anthropic
- endpoints:
    global:
      quotas:
        concurrent: null
        rpm:
          code: L-58424D95
          name: Global cross-region model inference requests per minute for Anthropic
            Claude Opus 4.5
        tpd:
          code: L-E2985A8E
          name: Global cross-region model inference tokens per day for Anthropic Claude
            Opus 4.5
        tpm:
          code: L-3ABF6ACC
          name: Global cross-region model inference tokens per minute for Anthropic
            Claude Opus 4.5
  inference_profiles:
  - global
  inference_types:
  - INFERENCE_PROFILE
  model_id: anthropic.claude-opus-4-5-20251101-v1:0
  provider: Anthropic
- endpoints:
    apac:
      quotas:
        concurrent: null
        rpm:
          code: L-559DCC33
          name: Cross-region model inference requests per minute for Anthropic Claude
            Sonnet 4 V1
        tpd:
          code: L-22F701C5
          name: Model invocation max tokens per day for Anthropic Claude Sonnet 4
            V1 (doubled for cross-region calls)
        tpm:
          code: L-59759B4A
          name: Cross-region model inference tokens per minute for Anthropic Claude
            Sonnet 4 V1
  inference_profiles:
  - apac
  inference_types:
  - INFERENCE_PROFILE
  model_id: anthropic.claude-sonnet-4-20250514-v1:0
  provider: Anthropic
- endpoints:
    global:
      quotas:
        concurrent: null
        rpm:
          code: L-DB84CE56
          name: Global cross-region model inference requests per minute for Anthropic
            Claude Sonnet 4.5 V1
        tpd:
          code: L-BC182137
          name: Global cross-region model inference tokens per day for Anthropic Claude
            Sonnet 4.5 V1
        tpm:
          code: L-27C57EE8
          name: Global cross-region model inference tokens per minute for Anthropic
            Claude Sonnet 4.5 V1
  inference_profiles:
  - global
  inference_types:
  - INFERENCE_PROFILE
  model_id: anthropic.claude-sonnet-4-5-20250929-v1:0
  provider: Anthropic
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm: null
        tpd: null
        tpm: null
  inference_types:
  - ON_DEMAND
  model_id: cohere.embed-english-v3
  provider: Cohere
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm: null
        tpd: null
        tpm: null
  inference_types:
  - ON_DEMAND
  model_id: cohere.embed-multilingual-v3
  provider: Cohere
- endpoints:
    global:
      quotas:
        concurrent: null
        rpm:
          code: L-7089DC7D
          name: Global cross-region model inference requests per minute for Cohere
            Embed V4
        tpd:
          code: L-795ADAB0
          name: Global cross-region model inference tokens per day for Cohere Embed
            V4
        tpm:
          code: L-02DFBB76
          name: Global cross-region model inference tokens per minute for Cohere Embed
            V4
  inference_profiles:
  - global
  inference_types:
  - INFERENCE_PROFILE
  model_id: cohere.embed-v4:0
  provider: Cohere
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-7A5451CB
          name: On-demand model inference requests per minute for DeepSeek V3 V1
        tpd: null
        tpm:
          code: L-30A6895A
          name: On-demand model inference tokens per minute for DeepSeek V3 V1
  inference_types:
  - ON_DEMAND
  model_id: deepseek.v3-v1:0
  provider: DeepSeek
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-999037CA
          name: On-demand model inference requests per minute for Gemma 3 12B
        tpd: null
        tpm:
          code: L-3FD4A73E
          name: On-demand model inference tokens per minute for Gemma 3 12B
  inference_types:
  - ON_DEMAND
  model_id: google.gemma-3-12b-it
  provider: Google
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-5D46C7AF
          name: On-demand model inference requests per minute for Gemma 3 27B
        tpd: null
        tpm:
          code: L-F8729E94
          name: On-demand model inference tokens per minute for Gemma 3 27B
  inference_types:
  - ON_DEMAND
  model_id: google.gemma-3-27b-it
  provider: Google
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-3056DF33
          name: On-demand model inference requests per minute for Gemma 3 4B
        tpd: null
        tpm:
          code: L-73FB8466
          name: On-demand model inference tokens per minute for Gemma 3 4B
  inference_types:
  - ON_DEMAND
  model_id: google.gemma-3-4b-it
  provider: Google
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-46D383AF
          name: On-demand model inference requests per minute for Meta Llama 3 70B
            Instruct
        tpd: null
        tpm:
          code: L-609E24B0
          name: On-demand model inference tokens per minute for Meta Llama 3 70B Instruct
  inference_types:
  - ON_DEMAND
  model_id: meta.llama3-70b-instruct-v1:0
  provider: Meta
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-320BEFEB
          name: On-demand model inference requests per minute for Meta Llama 3 8B
            Instruct
        tpd: null
        tpm:
          code: L-03A9B835
          name: On-demand model inference tokens per minute for Meta Llama 3 8B Instruct
  inference_types:
  - ON_DEMAND
  model_id: meta.llama3-8b-instruct-v1:0
  provider: Meta
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-828C986E
          name: On-demand model inference requests per minute for Minimax M2
        tpd: null
        tpm:
          code: L-A81B7C40
          name: On-demand model inference tokens per minute for Minimax M2
  inference_types:
  - ON_DEMAND
  model_id: minimax.minimax-m2
  provider: MiniMax
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-B14A9603
          name: On-demand model inference requests per minute for Magistral Small
            1.2
        tpd: null
        tpm:
          code: L-D18AC5F7
          name: On-demand model inference tokens per minute for Magistral Small 1.2
  inference_types:
  - ON_DEMAND
  model_id: mistral.magistral-small-2509
  provider: Mistral AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-99F7BDBC
          name: On-demand model inference requests per minute for Ministral 14B 3.0
        tpd: null
        tpm:
          code: L-334E5409
          name: On-demand model inference tokens per minute for Ministral 14B 3.0
  inference_types:
  - ON_DEMAND
  model_id: mistral.ministral-3-14b-instruct
  provider: Mistral AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-DCA37E91
          name: On-demand model inference requests per minute for Ministral 3B 3.0
        tpd: null
        tpm:
          code: L-8A4BEE90
          name: On-demand model inference tokens per minute for Ministral 3B 3.0
  inference_types:
  - ON_DEMAND
  model_id: mistral.ministral-3-3b-instruct
  provider: Mistral AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-2BDF9A55
          name: On-demand model inference requests per minute for Ministral 8B 3.0
        tpd: null
        tpm:
          code: L-3B98F300
          name: On-demand model inference tokens per minute for Ministral 8B 3.0
  inference_types:
  - ON_DEMAND
  model_id: mistral.ministral-3-8b-instruct
  provider: Mistral AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-D9A35062
          name: On-demand model inference requests per minute for Mistral 7B Instruct
        tpd: null
        tpm:
          code: L-02D831F1
          name: On-demand model inference tokens per minute for Mistral AI Mistral
            7B Instruct
  inference_types:
  - ON_DEMAND
  model_id: mistral.mistral-7b-instruct-v0:2
  provider: Mistral AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-3AF844DB
          name: On-demand model inference requests per minute for Mistral Large
        tpd: null
        tpm:
          code: L-01447289
          name: On-demand model inference tokens per minute for Mistral AI Mistral
            Large
  inference_types:
  - ON_DEMAND
  model_id: mistral.mistral-large-2402-v1:0
  provider: Mistral AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-5B274E24
          name: On-demand model inference requests per minute for Mistral Large 3
        tpd: null
        tpm:
          code: L-C709F563
          name: On-demand model inference tokens per minute for Mistral Large 3
  inference_types:
  - ON_DEMAND
  model_id: mistral.mistral-large-3-675b-instruct
  provider: Mistral AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-FD938632
          name: On-demand model inference requests per minute for Mistral Mixtral
            8x7b Instruct
        tpd: null
        tpm:
          code: L-490F4D1F
          name: On-demand model inference tokens per minute for Mistral AI Mixtral
            8X7BB Instruct
  inference_types:
  - ON_DEMAND
  model_id: mistral.mixtral-8x7b-instruct-v0:1
  provider: Mistral AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-17AE85BD
          name: On-demand model inference requests per minute for Voxtral Mini 1.0
        tpd: null
        tpm:
          code: L-0B767044
          name: On-demand model inference tokens per minute for Voxtral Mini 1.0
  inference_types:
  - ON_DEMAND
  model_id: mistral.voxtral-mini-3b-2507
  provider: Mistral AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-ACB2FB6A
          name: On-demand model inference requests per minute for Voxtral Small 1.0
        tpd: null
        tpm:
          code: L-930E2896
          name: On-demand model inference tokens per minute for Voxtral Small 1.0
  inference_types:
  - ON_DEMAND
  model_id: mistral.voxtral-small-24b-2507
  provider: Mistral AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-02572418
          name: On-demand model inference requests per minute for Kimi K2 Thinking
        tpd: null
        tpm:
          code: L-03579AC2
          name: On-demand model inference tokens per minute for Kimi K2 Thinking
  inference_types:
  - ON_DEMAND
  model_id: moonshot.kimi-k2-thinking
  provider: Moonshot AI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-AC7B3FB9
          name: On-demand model inference requests per minute for NVIDIA Nemotron
            Nano 2
        tpd: null
        tpm:
          code: L-33D3627D
          name: On-demand model inference tokens per minute for NVIDIA Nemotron Nano
            2
  inference_types:
  - ON_DEMAND
  model_id: nvidia.nemotron-nano-12b-v2
  provider: NVIDIA
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-2DA9C9CC
          name: On-demand model inference requests per minute for Nemotron Nano 3
            30B
        tpd: null
        tpm:
          code: L-C0663FA3
          name: On-demand model inference tokens per minute for Nemotron Nano 3 30B
  inference_types:
  - ON_DEMAND
  model_id: nvidia.nemotron-nano-3-30b
  provider: NVIDIA
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-AC7B3FB9
          name: On-demand model inference requests per minute for NVIDIA Nemotron
            Nano 2
        tpd: null
        tpm:
          code: L-33D3627D
          name: On-demand model inference tokens per minute for NVIDIA Nemotron Nano
            2
  inference_types:
  - ON_DEMAND
  model_id: nvidia.nemotron-nano-9b-v2
  provider: NVIDIA
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-25B50707
          name: On-demand model inference requests per minute for OpenAI GPT OSS 120B
        tpd: null
        tpm:
          code: L-9DC5F595
          name: On-demand model inference tokens per minute for OpenAI GPT OSS 120B
  inference_types:
  - ON_DEMAND
  model_id: openai.gpt-oss-120b-1:0
  provider: OpenAI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-AF7F0545
          name: On-demand model inference requests per minute for OpenAI GPT OSS 20B
        tpd: null
        tpm:
          code: L-036E14D8
          name: On-demand model inference tokens per minute for OpenAI GPT OSS 20B
  inference_types:
  - ON_DEMAND
  model_id: openai.gpt-oss-20b-1:0
  provider: OpenAI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm: null
        tpd: null
        tpm: null
  inference_types:
  - ON_DEMAND
  model_id: openai.gpt-oss-safeguard-120b
  provider: OpenAI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-65833D55
          name: On-demand model inference requests per minute for GPT OSS Safeguard
            20B
        tpd: null
        tpm:
          code: L-5D8F2F54
          name: On-demand model inference tokens per minute for GPT OSS Safeguard
            20B
  inference_types:
  - ON_DEMAND
  model_id: openai.gpt-oss-safeguard-20b
  provider: OpenAI
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-548A1A32
          name: On-demand model inference requests per minute for Qwen3 235B a22b
            2507 V1
        tpd: null
        tpm:
          code: L-3875BCCF
          name: On-demand model inference tokens per minute for Qwen3 235B a22b 2507
            V1
  inference_types:
  - ON_DEMAND
  model_id: qwen.qwen3-235b-a22b-2507-v1:0
  provider: Qwen
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-E880C759
          name: On-demand model inference requests per minute for Qwen3 32B V1
        tpd: null
        tpm:
          code: L-B7C52139
          name: On-demand model inference tokens per minute for Qwen3 32B V1
  inference_types:
  - ON_DEMAND
  model_id: qwen.qwen3-32b-v1:0
  provider: Qwen
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-66EE6E0B
          name: On-demand model inference requests per minute for Qwen3 Coder 30B
            a3b V1
        tpd: null
        tpm:
          code: L-92F81E14
          name: On-demand model inference tokens per minute for Qwen3 Coder 30B a3b
            V1
  inference_types:
  - ON_DEMAND
  model_id: qwen.qwen3-coder-30b-a3b-v1:0
  provider: Qwen
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-CBE7E3CD
          name: On-demand model inference requests per minute for Qwen3 Coder 480B
            a35b V1
        tpd: null
        tpm:
          code: L-4D0E44C8
          name: On-demand model inference tokens per minute for Qwen3 Coder 480B a35b
            V1
  inference_types:
  - ON_DEMAND
  model_id: qwen.qwen3-coder-480b-a35b-v1:0
  provider: Qwen
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-07B3CEEA
          name: On-demand model inference requests per minute for Qwen3 Next 80B A3B
        tpd: null
        tpm:
          code: L-37AB702E
          name: On-demand model inference tokens per minute for Qwen3 Next 80B A3B
  inference_types:
  - ON_DEMAND
  model_id: qwen.qwen3-next-80b-a3b
  provider: Qwen
- endpoints:
    base:
      quotas:
        concurrent: null
        rpm:
          code: L-11B56FB0
          name: On-demand model inference requests per minute for Qwen3 VL 235B A22B
        tpd: null
        tpm:
          code: L-46063925
          name: On-demand model inference tokens per minute for Qwen3 VL 235B A22B
  inference_types:
  - ON_DEMAND
  model_id: qwen.qwen3-vl-235b-a22b
  provider: Qwen
- endpoints:
    global:
      quotas:
        concurrent: null
        rpm: null
        tpd: null
        tpm: null
  inference_profiles:
  - global
  inference_types:
  - INFERENCE_PROFILE
  model_id: twelvelabs.pegasus-1-2-v1:0
  provider: TwelveLabs
